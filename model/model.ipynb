{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d7d6170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.5.1+cu121\n",
      "GPU: NVIDIA GeForce RTX 3060 Ti\n",
      "Dependencies installed successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import kagglehub\n",
    "import shutil\n",
    "import os\n",
    "from huggingface_hub import notebook_login\n",
    "import wandb\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from PIL import Image,ImageFile\n",
    "import os\n",
    "import csv\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForQuestionAnswering\n",
    "import os\n",
    "import accelerate\n",
    "import sys\n",
    "import subprocess\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected. Check your CUDA / GPU drivers.\")\n",
    "\n",
    "print(\"Dependencies installed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742f95e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 2. Login to Services\n",
    "\n",
    "print(\"--- Hugging Face Login ---\")\n",
    "print(\"You need a read-access token from https://huggingface.co/settings/tokens\")\n",
    "notebook_login()\n",
    "\n",
    "print(\"\\n--- Weights & Biases Login ---\")\n",
    "print(\"You need an API key from https://wandb.ai/authorize\")\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6216aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Run Baseline Inference\n",
    "\n",
    "# 1. Load SD1.5\n",
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "pipe = pipe.to(\"cuda\")\n",
    "\n",
    "# 2. Prompt for a mask\n",
    "prompt = \"Generate me a cover for an indie zombie video game. There is two zombie on the cover and something that looks like a maze\"\n",
    "\n",
    "# 3. Generate\n",
    "print(\"Generating baseline image...\")\n",
    "image = pipe(prompt).images[0]\n",
    "image.save(\"baseline_failure.png\")\n",
    "\n",
    "# 4. Display\n",
    "print(\"Baseline Result (Expected Failure):\")\n",
    "display(image)\n",
    "\n",
    "# 5. Clean up memory for training\n",
    "del pipe\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87109a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download\n",
    "path = kagglehub.dataset_download(\"veerpandya/video-game-covers\")\n",
    "print(\"Downloaded to:\", path)\n",
    "\n",
    "target_path = r\"D:\\Desktop\\Devoirs UTBM\\BR\\BR04\\AI54\\dataset\"\n",
    "os.makedirs(target_path, exist_ok=True)\n",
    "\n",
    "shutil.copytree(path, target_path, dirs_exist_ok=True)\n",
    "\n",
    "print(\"Moved dataset to:\", target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5677229",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_image_safe(path):\n",
    "    ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "    try:\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        return img\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to open {path}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7c8279",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:   0%|          | 19/5000 [00:11<48:00,  1.73it/s] d:\\Desktop\\Devoirs UTBM\\BR\\BR04\\AI54\\.venv\\lib\\site-packages\\PIL\\Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Processing images: 100%|██████████| 5000/5000 [50:30<00:00,  1.65it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Saved to captions.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "dataset_dir = \"dataset\"\n",
    "output_csv = \"captions.csv\"\n",
    "\n",
    "# Load BLIP\n",
    "model_id = \"Salesforce/blip-vqa-base\"\n",
    "processor = BlipProcessor.from_pretrained(model_id)\n",
    "model = BlipForQuestionAnswering.from_pretrained(model_id)\n",
    "\n",
    "def generate_caption(image_path):\n",
    "    img = open_image_safe(image_path)\n",
    "    if img is None:\n",
    "        return None\n",
    "\n",
    "    question = (\n",
    "        \"Describe this video game cover in rich visual detail. \"\n",
    "        \"Mention characters, creatures, actions, environment, objects, colors, mood, and visible text.\"\n",
    "    )\n",
    "\n",
    "    inputs = processor(img, question, return_tensors=\"pt\")\n",
    "    out = model.generate(**inputs, max_new_tokens=128)\n",
    "\n",
    "    caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "    return caption.strip()\n",
    "\n",
    "rows = []\n",
    "\n",
    "# Collect all image paths\n",
    "all_files = []\n",
    "for root, dirs, files in os.walk(dataset_dir):\n",
    "    for file in files:\n",
    "        if file.lower().endswith(('.png', '.jpg', '.jpeg')):  # optional filter\n",
    "            all_files.append(os.path.join(root, file))\n",
    "\n",
    "\n",
    "for img_path in tqdm(all_files, desc=\"Processing images\"):        \n",
    "        if img_path.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "            parts = img_path.split(\"\\\\\")\n",
    "            game_type = parts[2]\n",
    "            game_name = os.path.splitext(parts[-1])[0].replace(\"-\", \" \")\n",
    "\n",
    "            visual_desc = generate_caption(img_path)\n",
    "\n",
    "            final_caption = (\n",
    "                f\"This is a cover of a {game_type} video game named {game_name}. \"\n",
    "                f\"We can see on the cover: {visual_desc}\"\n",
    "            )\n",
    "\n",
    "            #print(\"Processed:\", img_path)\n",
    "            rows.append([img_path, final_caption])\n",
    "\n",
    "# Save CSV\n",
    "with open(output_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"img_path\", \"caption\"])\n",
    "    writer.writerows(rows)\n",
    "\n",
    "print(\"Done! Saved to\", output_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2342e024",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://raw.githubusercontent.com/huggingface/diffusers/v0.35.2/examples/text_to_image/train_text_to_image_lora.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cec6a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"MODEL_NAME\"] = \"runwayml/stable-diffusion-v1-5\"\n",
    "os.environ[\"TRAIN_DIR\"] = \"training_512\"\n",
    "os.environ[\"OUTPUT_DIR\"] = \"sd-indoor-segmentation-lora\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ac7600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data created in 'training_512' with updated captions.\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "original_dataset = \"dataset/dataset\"\n",
    "output_dir = \"training_512\"\n",
    "captions_file = os.path.join(original_dataset, \"captions.csv\")\n",
    "output_captions_file = os.path.join(output_dir, \"metadata.csv\")\n",
    "\n",
    "def strip_png_metadata(path):\n",
    "    try:\n",
    "        with Image.open(path) as img:\n",
    "            # Convert image mode to RGB or RGBA\n",
    "            img = img.convert(\"RGBA\") if img.mode in [\"RGBA\", \"LA\"] else img.convert(\"RGB\")\n",
    "            \n",
    "            # Overwrite the original file without metadata\n",
    "            img.save(path, optimize=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to clean {path}: {e}\")\n",
    "        # fallback: try using ImageFile.LOAD_TRUNCATED_IMAGES\n",
    "        from PIL import ImageFile\n",
    "        ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "        try:\n",
    "            with Image.open(path) as img:\n",
    "                img = img.convert(\"RGBA\") if img.mode in [\"RGBA\", \"LA\"] else img.convert(\"RGB\")\n",
    "                img.save(path, optimize=True)\n",
    "                print(f\"Cleaned with fallback: {path}\")\n",
    "        except Exception as e2:\n",
    "            print(f\"Failed again: {path}, {e2}\")\n",
    "\n",
    "def resize(input_path,output_path):\n",
    "    target_size = (512, 512)\n",
    "\n",
    "    # Open and resize image\n",
    "    with Image.open(input_path) as img:\n",
    "        resized_img = img.resize(target_size, Image.LANCZOS)\n",
    "        resized_img.save(output_path)\n",
    "\n",
    "# Create output folder if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load captions\n",
    "captions_df = pd.read_csv(captions_file, names=['img_path', 'caption'])\n",
    "#print(captions_df)\n",
    "# Prepare a list to store new captions\n",
    "new_captions = []\n",
    "\n",
    "# Loop through all subfolders\n",
    "for root, _, files in os.walk(original_dataset):\n",
    "    for filename in files:\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            old_path = os.path.join(root, filename)\n",
    "            \n",
    "            # Copy file to output_dir\n",
    "            new_filename = filename\n",
    "            counter = 1\n",
    "            while os.path.exists(os.path.join(output_dir, new_filename)):\n",
    "                name, ext = os.path.splitext(filename)\n",
    "                new_filename = f\"{name}_{counter}{ext}\"\n",
    "                counter += 1\n",
    "            strip_png_metadata(old_path)\n",
    "            resize(old_path,os.path.join(output_dir, new_filename))\n",
    "            \n",
    "            # Update captions\n",
    "            old_path = old_path.replace(\"\\\\\", \"/\").replace(\"/\",\"\\\\\")\n",
    "            row = captions_df[captions_df['img_path'] == old_path]\n",
    "            if not row.empty:\n",
    "                row = row.copy()\n",
    "                row['img_path'] = new_filename\n",
    "                new_captions.append(row)\n",
    "\n",
    "# Combine all updated captions and save\n",
    "if new_captions:\n",
    "    new_captions_df = pd.concat(new_captions, ignore_index=True)\n",
    "    new_captions_df = new_captions_df.rename(columns={\n",
    "        'img_path': 'file_name',\n",
    "        'caption': 'label'\n",
    "    })\n",
    "    new_captions_df.to_csv(output_captions_file, index=False, header=True)\n",
    "    print(f\"Training data created in '{output_dir}' with updated captions.\")\n",
    "else:\n",
    "    print(\"No captions matched. Check your captions.csv paths.\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca7b64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!accelerate launch train_text_to_image_lora.py --pretrained_model_name_or_path runwayml/stable-diffusion-v1-5 --train_data_dir training_512/ --caption_column label --resolution 512 --train_batch_size 1 --gradient_accumulation_steps 4 --max_train_steps 2000 --learning_rate 1e-4 --max_grad_norm 1 --lr_scheduler cosine --lr_warmup_steps 0 --output_dir sd-indoor-segmentation-lora --mixed_precision fp16 --seed 42 --report_to tensorboard --validation_prompt \"Generate me an indie cover for a video game\" --checkpointing_steps 500 --checkpoints_total_limit 2 --rank 64  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c268511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Run LoRA Inference\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from PIL import Image\n",
    "\n",
    "# 1. Load Base Model\n",
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "pipe = pipe.to(\"cuda\")\n",
    "\n",
    "# 2. Load the LoRA weights\n",
    "lora_path = r\"sd-indoor-segmentation-lora\\pytorch_lora_weights.safetensors\"\n",
    "pipe.load_lora_weights(lora_path)\n",
    "\n",
    "# 3. Prompt matches the style used in training\n",
    "prompt = \"Generate me a cover for an indie zombie video game. There is two zombie on the cover and something that looks like a maze\"\n",
    "\n",
    "# 4. Generate\n",
    "print(\"Generating LoRA result...\")\n",
    "image = pipe(prompt, num_inference_steps=30).images[0]\n",
    "\n",
    "# 5. Display and Save\n",
    "image.save(\"lora_final_result.png\")\n",
    "display(image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
