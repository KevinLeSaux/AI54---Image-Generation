{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "14d07a3fa65048349fa91a7b58c5867b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6d8f58c141d9469ab9098898bc6f63b4",
              "IPY_MODEL_1af6cde7a3a449979a9e557eef4dcec0",
              "IPY_MODEL_9c7c4c98c07f4913ab8d3b754d50d78f"
            ],
            "layout": "IPY_MODEL_7102909866234b0b82fe4a3471b043a7"
          }
        },
        "6d8f58c141d9469ab9098898bc6f63b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1d999cb6bcd40b18b2bceb2c32bf8f2",
            "placeholder": "​",
            "style": "IPY_MODEL_c60240c6a1244fa8b92a1cf597c2c006",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "1af6cde7a3a449979a9e557eef4dcec0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b74bb40c44b44cead3a717d689683d4",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3a0d117a9ce94dc69304262900550ace",
            "value": 4
          }
        },
        "9c7c4c98c07f4913ab8d3b754d50d78f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b717979fe39f4ea3b10dc529236e7cca",
            "placeholder": "​",
            "style": "IPY_MODEL_970b3e8160d4449990082c17d03f4b9f",
            "value": " 4/4 [01:22&lt;00:00, 18.17s/it]"
          }
        },
        "7102909866234b0b82fe4a3471b043a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1d999cb6bcd40b18b2bceb2c32bf8f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c60240c6a1244fa8b92a1cf597c2c006": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9b74bb40c44b44cead3a717d689683d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a0d117a9ce94dc69304262900550ace": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b717979fe39f4ea3b10dc529236e7cca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "970b3e8160d4449990082c17d03f4b9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%env PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True"
      ],
      "metadata": {
        "id": "rmt7P90xYUA6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6031c0ca-9c0e-4224-a1c8-06704169354c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
        "import gc\n",
        "import csv\n",
        "import os"
      ],
      "metadata": {
        "id": "CwmnfChao3X3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "zjN8-P-4qQAb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "PROMPT = \"Write a long descriptive caption for this image in a formal tone.\"\n",
        "MODEL_NAME = \"fancyfeast/llama-joycaption-beta-one-hf-llava\"\n"
      ],
      "metadata": {
        "id": "VzRqEckeYVQE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load JoyCaption\n",
        "# bfloat16 is the native dtype of the LLM used in JoyCaption (Llama 3.1)\n",
        "# device_map=0 loads the model into the first GPU\n",
        "processor = AutoProcessor.from_pretrained(MODEL_NAME)\n",
        "llava_model = LlavaForConditionalGeneration.from_pretrained(MODEL_NAME, torch_dtype=\"float16\", device_map=\"auto\")\n",
        "llava_model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "14d07a3fa65048349fa91a7b58c5867b",
            "6d8f58c141d9469ab9098898bc6f63b4",
            "1af6cde7a3a449979a9e557eef4dcec0",
            "9c7c4c98c07f4913ab8d3b754d50d78f",
            "7102909866234b0b82fe4a3471b043a7",
            "b1d999cb6bcd40b18b2bceb2c32bf8f2",
            "c60240c6a1244fa8b92a1cf597c2c006",
            "9b74bb40c44b44cead3a717d689683d4",
            "3a0d117a9ce94dc69304262900550ace",
            "b717979fe39f4ea3b10dc529236e7cca",
            "970b3e8160d4449990082c17d03f4b9f"
          ]
        },
        "collapsed": true,
        "id": "GoxD3Zc6nq2n",
        "outputId": "ad77b155-631d-4330-f801-1f3a4fce10a1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "14d07a3fa65048349fa91a7b58c5867b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LlavaForConditionalGeneration(\n",
              "  (model): LlavaModel(\n",
              "    (vision_tower): SiglipVisionModel(\n",
              "      (vision_model): SiglipVisionTransformer(\n",
              "        (embeddings): SiglipVisionEmbeddings(\n",
              "          (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
              "          (position_embedding): Embedding(729, 1152)\n",
              "        )\n",
              "        (encoder): SiglipEncoder(\n",
              "          (layers): ModuleList(\n",
              "            (0-26): 27 x SiglipEncoderLayer(\n",
              "              (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "              (self_attn): SiglipAttention(\n",
              "                (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
              "                (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
              "                (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
              "                (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
              "              )\n",
              "              (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "              (mlp): SiglipMLP(\n",
              "                (activation_fn): GELUTanh()\n",
              "                (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
              "                (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "        (head): SiglipMultiheadAttentionPoolingHead(\n",
              "          (attention): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1152, out_features=1152, bias=True)\n",
              "          )\n",
              "          (layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): SiglipMLP(\n",
              "            (activation_fn): GELUTanh()\n",
              "            (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
              "            (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (multi_modal_projector): LlavaMultiModalProjector(\n",
              "      (linear_1): Linear(in_features=1152, out_features=4096, bias=True)\n",
              "      (act): GELUActivation()\n",
              "      (linear_2): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    )\n",
              "    (language_model): LlamaModel(\n",
              "      (embed_tokens): Embedding(128256, 4096)\n",
              "      (layers): ModuleList(\n",
              "        (0-31): 32 x LlamaDecoderLayer(\n",
              "          (self_attn): LlamaAttention(\n",
              "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          )\n",
              "          (mlp): LlamaMLP(\n",
              "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
              "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
              "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
              "            (act_fn): SiLUActivation()\n",
              "          )\n",
              "          (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "          (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "        )\n",
              "      )\n",
              "      (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "      (rotary_emb): LlamaRotaryEmbedding()\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_DIR = \"test\"\n",
        "OUTPUT_CSV = \"captions.csv\"\n",
        "results = []\n",
        "\n",
        "# Build the conversation\n",
        "convo = [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are a helpful image captioner.\",\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": PROMPT,\n",
        "            },\n",
        "        ]\n",
        "# Format the conversation\n",
        "        # WARNING: HF's handling of chat's on Llava models is very fragile.  This specific combination of processor.apply_chat_template(), and processor() works\n",
        "        # but if using other combinations always inspect the final input_ids to ensure they are correct.  Often times you will end up with multiple <bos> tokens\n",
        "        # if not careful, which can make the model perform poorly.\n",
        "convo_string = processor.apply_chat_template(convo, tokenize = False, add_generation_prompt = True)\n",
        "assert isinstance(convo_string, str)\n",
        "\n",
        "with torch.no_grad():\n",
        "  for filename in os.listdir(DATASET_DIR):\n",
        "        if not filename.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
        "            continue  # skip non-image files\n",
        "        # Load image\n",
        "        image_path = os.path.join(DATASET_DIR, filename)\n",
        "        image = Image.open(image_path)\n",
        "\n",
        "\n",
        "        # Process the inputs\n",
        "        inputs = processor(text=[convo_string], images=[image], return_tensors=\"pt\").to('cuda')\n",
        "        inputs['pixel_values'] = inputs['pixel_values'].to(torch.float16)\n",
        "\n",
        "        # Generate the captions\n",
        "        generate_ids = llava_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=512,\n",
        "            do_sample=True,\n",
        "            suppress_tokens=None,\n",
        "            use_cache=True,\n",
        "            temperature=0.6,\n",
        "            top_k=None,\n",
        "            top_p=0.9,\n",
        "        )[0]\n",
        "\n",
        "        # Trim off the prompt\n",
        "        generate_ids = generate_ids[inputs['input_ids'].shape[1]:]\n",
        "\n",
        "        # Decode the caption\n",
        "        caption = processor.tokenizer.decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
        "        caption = caption.strip()\n",
        "\n",
        "        print(f\"{filename}: {caption}\")\n",
        "        results.append([filename, caption])\n",
        "\n",
        "    # Save results to CSV\n",
        "with open(OUTPUT_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([\"filename\", \"caption\"])  # header\n",
        "    writer.writerows(results)\n",
        "\n",
        "print(f\"Saved {len(results)} captions to {OUTPUT_CSV}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rcGRiZQt6s3",
        "outputId": "4617fd50-a611-4926-f4c9-5a482db5a7e8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "102-dalmatians-puppies-to-the-rescue--1.png: This is a digitally created movie poster for \"102 Dalmatians: puppies to the rescue.\" The foreground features two animated Dalmatian puppies with white fur and black spots, one with a red collar and a tag, and the other with a pink collar. Both puppies have expressive, cheerful faces. The background shows a large, menacing, animated figure of Cruella de Vil with an orange, wavy hairstyle and a sinister grin. The iconic Big Ben clock tower is visible to the right, set against a twilight sky. The title \"102 Dalmatians\" is prominently displayed in bold, white, and red letters with black spots, and \"Puppies to the Rescue\" is written in blue beneath it. The Disney logo is on the top left. The overall style is colorful and cartoonish.\n",
            "asterix.png: This is a digital cartoon illustration featuring Asterix, a character from the popular French comic series. Asterix, a white anthropomorphic Gaul with white wings, blonde hair, and a large nose, is depicted in a dynamic, mid-dance pose with outstretched arms. He wears a black sleeveless shirt, red pants, and yellow shoes, with a green belt adorned with yellow circular ornaments. The background is a vibrant gradient from red to orange, depicting a cheering crowd and ancient Roman architecture. The name \"Asterix\" is prominently displayed in bold red letters at the top. The overall style is colorful and energetic, typical of classic comic book art.\n",
            "Saved 2 captions to captions.csv\n"
          ]
        }
      ]
    }
  ]
}